```{r setup, include=FALSE}
require(ggplot2)
require(readr)

theme_set(theme_grey(base_size=20))

# Covariance matrix for noise "like" the given residuals.
#
# Assumes they were generated from a stationary process.
EmpiricalCovarianceMatrix <- function(residuals) {
  autocorrelation <- acf(residuals, lag.max=length(residuals))$acf
  sigma_sq <- var(residuals)
  return (outer(1:length(residuals), 1:length(residuals),
                function(a, b) { sigma_sq * autocorrelation[abs(a - b) + 1] }))
}

# A matrix whose columns are the values of the supplied basis functions,
# evaluated at the supplied points x.
LinearLeastSquaresMatrix <- function(x, functions) {
  sapply(functions,
         function(f) {
           sapply(x, function(x) { f(x) })
         })
}

# A matrix for the disconnected linear model.
DisconnectedLinearMatrix <- function(x, x_break) {
  functions <- list(function(z) 1 * (z < x_break),
                    function(z) as.numeric(z) * (z < x_break),
                    function(z) 1 * (z > x_break),
                    function(z) as.numeric(z) * (z > x_break))
  LinearLeastSquaresMatrix(x, functions)
}

# A matrix for the disconnected linear model.
ConnectedLinearMatrix <- function(x, x_break) {
  functions <- list(function(z) 1,
                    function(z) z,
                    function(z) as.numeric(z - x_break) * (z > x_break))
  LinearLeastSquaresMatrix(x, functions)
}

# A matrix for a single trendline model.
SingleLinearMatrix <- function(x) {
  functions <- list(function(z) 1,
                    function(z) z)
  LinearLeastSquaresMatrix(x, functions)
}

# The "best fit" value (for the given model) for each y value.
#
# Args:
#   y:  A vector of datapoints.
#   model:  A model matrix: each column gives the value of one of the basis
#     functions, evaluated at the same position as the corresponding y value.
Fit <- function(y, model) {
  model %*% solve(t(model) %*% model) %*% t(model) %*% y
}

# Build a data.frame from an RSS CSV data file.
#
# Args:
#   rss_file_name:  The name of the RSS CSV file (columns 'year', 'month',
#     'anomaly').
#   cutoff_date:  The date when the rate of warming is supposed to have
#     changed.
RssDataset <- function(rss_file_name, cutoff_date) {
  # Read the filename.
  d <- read_csv(rss_file_name)
  # Compute a proper 'date' column.
  d$date <- as.Date(sprintf("%04d-%02d-01", d$year, d$month))

  # Compute the fit points for a "single trendline" model.
  d$single <- Fit(d$anomaly,
                  SingleLinearMatrix(d$date))

  # Compute the fit points for the "disconnected trendlines" model.
  d$disconnected <- Fit(d$anomaly,
                        DisconnectedLinearMatrix(d$date, cutoff_date))

  # Compute the fit points for the "connected trendlines" model.
  d$connected <- Fit(d$anomaly,
                     ConnectedLinearMatrix(d$date, cutoff_date))

  return (d)
}

# The number of degrees per century between the earliest and latest dates which
# pass the filter.
#
# Args:
#   dates:  A vector of dates.
#   y:  A vector of values (presumably temperatures or temperature anomalies),
#     one for each value in dates.
#   filter:  A boolean function which accepts a date.
DegreesPerCenturyWithFilter <- function(dates, y, filter) {
  i_ok <- which(filter(dates))
  i_extreme <- which(dates %in% range(dates[i_ok]))
  days_per_century <- 36524.25
  diff_days <- diff(as.numeric(dates[i_extreme]))
  return (diff(y[i_extreme]) / (diff_days / days_per_century)) 
}

# The number of degrees per century between the earliest and latest dates.
#
# Args:
#   dates:  A vector of dates.
#   y:  A vector of values (presumably temperatures or temperature anomalies),
#     one for each value in dates.
DegreesPerCentury <- function(dates, y) {
  DegreesPerCenturyWithFilter(dates, y, function(d) as.logical(d) | TRUE)
}

# How many years of warming (at a given rate) to get a change in temperature?
YearsOfWarming <- function(temp_change, rate_per_century) {
  temp_change / rate_per_century * 100
}

# Append a 'degrees C' to a temperature
PrintTemp <- function(temperature) {
  sprintf('%4.2fÂ°C', temperature)
}

# Print the trend in human-readable form: round to 2 decimal places, and label
# as degrees per century.
PrintTrend <- function(temperature, time_span='century') {
  sprintf('%s / %s', PrintTemp(temperature), time_span)
}

# A geom_text to label a line.
TextLabel <- function(label, d, column, ...) {
  geom_text(label=label,
            data=d[round((1 + nrow(d)) / 2), ],
            aes_string(x='date', y=column),
            hjust=0, vjust=1.5, size=10, ...)
}

# A geom_line to show a fit.
FitLine <- function(d, column, ...) {
  geom_line(data=d, aes_string(x='date', y=column), size=1.2, ...)
}

# The (first) index of the element in y whose following element increases by
# the most.
BiggestJumpIndex <- function(y) {
  dy <- diff(y)
  head(which(dy == max(dy)), 1)
}

# Cutoff date variable, and associated functions.
cutoff_date <- as.Date("1997-02-15")
Before <- function(d) d < cutoff_date
After <- function(d) d > cutoff_date

# Construct the RSS data frame.
rss_file <- 'content/blog/connect_your_trendlines/data/rss.csv'
d_rss <- RssDataset(rss_file, cutoff_date)

single_trendline_slope <- DegreesPerCentury(d_rss$date, d_rss$single)
bad_slope_before <- DegreesPerCenturyWithFilter(d_rss$date, d_rss$disconnected,
                                                Before)
bad_slope_after <- DegreesPerCenturyWithFilter(d_rss$date, d_rss$disconnected,
                                               After)
good_slope_before <- DegreesPerCenturyWithFilter(d_rss$date, d_rss$connected,
                                                 Before)
good_slope_after <- DegreesPerCenturyWithFilter(d_rss$date, d_rss$connected,
                                                After)
jump <- max(diff(d_rss$disconnected))
```

```{r pause_hunting, include=FALSE, cache=TRUE}
last_date <- tail(d_rss$date, 1)
seen_negative_slope <- FALSE
for (pause_date in seq(from=last_date - 45, to=head(d_rss$date, 1),
                       by='-1 month')) {
  connected_fit <- Fit(d_rss$anomaly,
                       ConnectedLinearMatrix(d_rss$date, pause_date))
  post_cutoff_slope <- DegreesPerCenturyWithFilter(d_rss$date, connected_fit,
                                                   function(d) d > pause_date)
  if (seen_negative_slope && post_cutoff_slope > 0) {
    break
  }
  seen_negative_slope <- seen_negative_slope || (post_cutoff_slope < 0)
}

# It is annoying that I have to do this.  Why doesn't a sequence of Date
# objects automatically contain Date objects?
pause_date <- as.Date(pause_date, origin='1970-01-01')

# Compute the fit and slopes for this pause.
d_rss$newpause <- Fit(d_rss$anomaly,
                      ConnectedLinearMatrix(d_rss$date, pause_date))
pause_slope_before <- DegreesPerCenturyWithFilter(d_rss$date, d_rss$newpause,
                                                  function(d) d < pause_date)
pause_slope_after <- DegreesPerCenturyWithFilter(d_rss$date, d_rss$newpause,
                                                  function(d) d > pause_date)

# Quick-and-dirty utility functions to help answer the question, "how long, in
# months, from one date to another?" where we only look at the year and month
# of each date.
Month <- function(date) as.numeric(format(date, '%m'))
Year <- function(date) as.numeric(format(date, '%Y'))
MonthsSpan <- function(from, to) {
  12 * (Year(to) - Year(from)) + Month(to) - Month(from)
}
PrintMonthsSpanInYears <- function(from, to) {
  months <- MonthsSpan(from, to)
  sprintf('%d years and %d months', months %/% 12, months %% 12)
}
```

The global warming "pause" means different things to different people.

- Have the climate models overestimated the recent warming?
- Has global warming slowed down?
- Has global warming *stopped*?

All these questions are interesting and important, but this post will focus only on the last one.

Climate skeptic [Christopher Monckton claims](http://www.climatedepot.com/2015/11/04/no-global-warming-at-all-for-18-years-9-months-a-new-record-the-pause-lengthens-again-just-in-time-for-un-summit-in-paris/) that global warming stopped more than 18 years ago.
He bases this claim on a satellite temperature dataset from the Remote Sensing Systems company ("RSS" for short).
Scientists have responded in various ways.
One explanation is that lately, more heat has been going into the ocean instead of the atmosphere.
Another is that other, land-based datasets (which do not show such an extended pause) more accurately reflect reality.

These sophisticated responses can be hard to evaluate without expert domain knowledge.
Here, I'll take a different approach: I'll look directly at the RSS data, taking them at face value for this post.

It turns out that these data do *not* show an 18-year pause.
The model that says they do is broken: to get the 18 warming-free years, it implicitly *crams 18 years' worth of warming into a single instantaneous jump*.

Fortunately, the model is easy to fix.
When we do, we find an apparent pause of `r PrintMonthsSpanInYears(pause_date, last_date)`.
This is short enough to be explained by natural variability, which means that any talk of a "global warming stopped" pause is at best premature.

But let's begin by looking at the raw data themselves.

## The raw RSS data

```{r bare_rss, echo=FALSE}
p <- (ggplot(data=d_rss, aes(x=date))
      + geom_line(colour='#888888', aes(y=anomaly))
      + ggtitle('RSS Satellite Temperature Record')
      + scale_x_date('Date')
      + scale_y_continuous('Temperature Anomaly (K)')
      )
print(p)
```

These data fluctuate -- a *lot*.
The fluctuations represent actual physical events, but for our purposes they're just noise: we're looking for underlying *trends* in the data.

Not much is clear at first glance.
Maybe the later temperatures are overall higher than the earlier data?
It's hard to be sure by eyeball: what we need is a *model*.

We'll try out different models in this post, using the standard [linear least squares](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)) technique to find the best fit in each case.

## The simplest model: one trendline

One approach is to assume that a single, constant trend can describe the data.
That trend is the *slope* of the line of best fit: here, we find `r PrintTrend(single_trendline_slope)`.

```{r single_trendline, echo=FALSE}
colour_single <- 'purple'
column_single <- 'single'
line_single <- FitLine(d=d_rss, column=column_single, colour=colour_single)
label_single <- TextLabel(PrintTrend(single_trendline_slope), d_rss, 'single',
                          colour=colour_single)
print(p
      + line_single
      + label_single
      + ggtitle('RSS fit: single trendline')
      )
```

This model is exceedingly simple -- and that is its strength!
More complicated models risk mistaking noise for signal.
This phenomenon is called "overfitting", and we want to avoid it.

Of course, the downside to a simple model is that it may be *too* simple to capture important changes.

## The next model: let the trend change

Suppose the underlying trend *changes*.
Our model will still give an answer, but it will be wrong.
We need a more sophisticated model to check whether the trend has changed.

(Note that it is very important to be able to detect trendline changes.
After all, changing the trend is *exactly what environmentalists are trying to do!*)

In particular, Lord Monckton suspects that the trend has completely flattened (i.e., global warming has stopped).
He looks for the longest stretch of time which yields a flat trend up to the present day, by the same method we used above (linear least squares).
His answer is shockingly long: *no global warming for 18 years*!

```{r partial_disconnected, echo=FALSE}
colour_disc <- 'blue'
column_disc <- 'disconnected'
line_disc <- FitLine(d=d_rss, column=column_disc, colour=colour_disc)
label_disc <- TextLabel(PrintTrend(bad_slope_after), d_rss, column_disc,
                        colour=colour_disc)
p_disconnected <- (p
                   + line_disc
                   + label_disc
                   + ggtitle('RSS fit (latest 18 years only):\n"No" Global Warming!')
                   )
print(p_disconnected + coord_cartesian(xlim=c(cutoff_date + 30, max(d_rss$date))))
```


Actually, it's not *exactly* the same method; he has excluded the earlier dates.
When we add them back in, we discover a flaw in this model.

```{r full_disconnected, echo=FALSE}
i_jump <- BiggestJumpIndex(d_rss$disconnected)
column_jump <- column_disc
colour_jump <- 'red'
print(p_disconnected
      + ggtitle('Including all the data:\nInstantaneous temperature jump')
      + TextLabel(PrintTrend(bad_slope_before), d_rss[1, ], column_disc,
                  colour=colour_disc)
      + FitLine(d=d_rss[i_jump + 0:1, ], column=column_jump,
                colour=colour_jump)
      + TextLabel(sprintf('+%s!', PrintTemp(jump)), d_rss[i_jump, ],
                  column_jump, colour=colour_jump)
      )
```


We do see that the slope has changed (from `r PrintTemp(bad_slope_before)` to `r PrintTrend(bad_slope_after)`).
But we also see something else: the temperature *instantaneously jumps*[^escalator] by `r PrintTemp(jump)` at the transition!
This is not a small jump, either: it is equivalent to `r round(YearsOfWarming(jump, single_trendline_slope))` years' worth of warming at the single-trendline rate of `r PrintTrend(single_trendline_slope)`.

So the 18-year "pause" really hides 18 years of warming in a single instant.

[^escalator]: This unphysical temperature jump is implicit in other analyses (such as the [escalator animation](http://www.skepticalscience.com/graphics.php?g=47)), but in this post I wanted to highlight it explicitly.

## Fixing the model: trendlines done *right*

Everybody agrees that the *real* temperature trend doesn't have any "jumps".
So why not bake this assumption into the model?

```{r connected, echo=FALSE}
colour_conn <- 'red'
column_conn <- 'connected'
line_conn <- FitLine(d=d_rss, column=column_conn, colour=colour_conn)
label_conn_before <- TextLabel(PrintTrend(good_slope_before),
                               d_rss[nrow(d_rss)*0.15, ], column_conn,
                               colour=colour_conn)
label_conn_after <- TextLabel(PrintTrend(good_slope_after),
                              d_rss[nrow(d_rss)*0.65, ], column_conn,
                              colour=colour_conn)

p_connected <- (p
                + line_conn
                + label_conn_before
                + label_conn_after
                + ggtitle('"Changing trendlines" model:\nthe correct (i.e., connected) version')
                )
print(p_connected)
```

The *improved* model still allows the trend to change; it just makes sure to *connect the trendlines*.
We see that the warming is alive and well: `r PrintTrend(good_slope_after)`.

This is somewhat smaller than the average slope from the simplest model (`r PrintTrend(single_trendline_slope)`)
However, it's actually *bigger* than the biggest slope from the disconnected model (`r PrintTrend(bad_slope_before)`)!
This is another effect of squeezing all the warming into a single instant.
The model underestimates the warming *before* as well as after.

## What's the longest pause with the *fixed* model?

We can play the same game as Lord Monckton, but using the fixed version of his model.
How far back can we go and find a zero-slope trendline *which connects to the previous trendline*?

```{r connected_pause, echo=FALSE}
colour_newpause <- '#884400'
column_newpause <- 'newpause'
line_newpause <- FitLine(d=d_rss, column=column_newpause,
                         colour=colour_newpause)
label_newpause_before <- TextLabel(PrintTrend(pause_slope_before),
                               d_rss[nrow(d_rss)*0.15, ], column_newpause,
                               colour=colour_newpause)
label_newpause_after <- TextLabel(PrintTrend(pause_slope_after),
                              d_rss[nrow(d_rss)*0.65, ], column_newpause,
                              colour=colour_newpause)
print(p
      + line_newpause
      + label_newpause_before
      + label_newpause_after
      + ggtitle(sprintf("The longest 'pause' with a credible model:\n%s",
                        PrintMonthsSpanInYears(pause_date, last_date)))
      )
```

```{r all_models, echo=FALSE}
p_all <- (p
          + line_single
          + line_disc
          + line_conn
          + line_newpause
          + ggtitle('All models together')
          )
# Don't print this plot; I can't figure out how to fit it into the narrative without rambling even more.
```

The answer: `r PrintMonthsSpanInYears(pause_date, last_date)`.
This is not completely negligible, but it's a lot shorter than `r PrintMonthsSpanInYears(cutoff_date, last_date)`.

In particular, it's now well within the range of what natural variability can explain.
Judith Curry (a climate scientist with climate-skeptic leanings) [gives 17 years](http://judithcurry.com/2015/11/06/hiatus-controversy-show-me-the-data/) as a threshold to rule out explanations based on natural variability.
Since fixing the model has "rewound the pause", we are now well within that threshold.
It looks like we'll need to wait at least `r 17 - MonthsSpan(pause_date, last_date) %/% 12` years before we can claim that global warming has stopped.

Of course, that's only if the data continue to "cooperate".
With 2015 [shaping up to be a real scorcher](https://www.washingtonpost.com/news/capital-weather-gang/wp/2015/10/21/after-record-shattering-september-2015-in-commanding-lead-for-earths-hottest-year-on-record/), I doubt we'll ever see a true pause (of this kind) materialize!
